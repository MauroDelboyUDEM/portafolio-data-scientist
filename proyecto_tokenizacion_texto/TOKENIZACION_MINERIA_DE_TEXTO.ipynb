{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec500651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.12/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (1.3.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567d0ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maurodelboycespedes/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar lo recursos necesarios\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "226b6ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', ',', '¿cómo', 'estás', '?', 'Que', 'tengas', 'un', 'excelente', 'día', 'de', 'San', 'Valentín', '.']\n"
     ]
    }
   ],
   "source": [
    "# Minería de texto\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"Hola, ¿cómo estás? Que tengas un excelente día de San Valentín.\"\n",
    "# Tokenizar esta frase\n",
    "\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f1a1905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola',\n",
       " '¿cómo',\n",
       " 'estás',\n",
       " 'Que',\n",
       " 'tengas',\n",
       " 'un',\n",
       " 'excelente',\n",
       " 'día',\n",
       " 'de',\n",
       " 'San',\n",
       " 'Valentín']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sin signos de puntuación\n",
    "import string\n",
    "\n",
    "tokens_filtrados = [palabra for palabra in tokens if palabra not in string.punctuation]\n",
    "tokens_filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1648b330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'precio', 'es', 'de', '83.99', 'pesos', 'y', 'la', 'oferta', 'termina', 'el', '14/02/2025', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar textos numéricos\n",
    "text_num = \"El precio es de 83.99 pesos y la oferta termina el 14/02/2025.\"\n",
    "tokens_num = word_tokenize(text_num)\n",
    "print(tokens_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e88cee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'I', \"'m\", 'learning', 'Artificial', 'Intelligence', 'and', 'Data', 'Science', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos en inglés \n",
    "texto = \"Hello! I'm learning Artificial Intelligence and Data Science.\"\n",
    "tokens_en = word_tokenize(texto)\n",
    "\n",
    "\n",
    "print(tokens_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcaeacf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maurodelboycespedes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9721359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about', 'few', 'so', 'needn', 'having', 'hasn', 'which', 'too', 's', 'can', \"wasn't\", 'are', 'while', \"she's\", 'where', 'because', \"mustn't\", 'if', 'through', 'm', 'a', 'doesn', 'themselves', 'most', 'between', 'ours', 'yours', \"it's\", \"don't\", \"you've\", 'did', 'same', 'won', 'or', 'each', 'into', 'on', 'until', 'but', 'no', 'isn', \"that'll\", 'been', 'shan', 'than', 'there', 'hers', 'wouldn', \"you're\", 'he', 'him', 'before', \"won't\", \"needn't\", \"doesn't\", 'was', 'do', 'being', 'both', 'd', 'whom', \"wouldn't\", 'from', 'such', 'his', 'with', 'weren', 'an', 'hadn', 'we', 'don', 'nor', 'my', \"you'll\", 'herself', 're', 'not', \"should've\", \"haven't\", 'over', 'be', 't', 'its', 'only', 'your', 'why', 'theirs', 'any', 'is', 'these', 'under', 'all', 'above', 'you', 'her', 'further', 'just', 'aren', 'by', \"shan't\", 'when', 'once', 'o', 'were', 'wasn', 'down', \"couldn't\", 'me', 'for', 'yourself', 'had', 'at', 'and', 'ourselves', 'myself', 'of', 'will', 'how', 'she', 'ain', 'it', 'them', 'what', 'ma', 'mustn', 'off', 'does', 'doing', 'am', \"hasn't\", 'mightn', 'then', 'didn', 'other', \"shouldn't\", 'to', \"you'd\", 'very', 've', \"hadn't\", 'more', 'as', 'some', 'has', 'our', 'below', 'out', 'who', 'll', 'the', 'after', \"didn't\", 'that', 'yourselves', 'they', 'have', \"isn't\", \"mightn't\", 'here', 'against', 'their', 'up', 'now', 'those', \"aren't\", 'shouldn', 'himself', 'in', 'again', 'haven', 'should', 'i', 'during', 'this', 'own', 'y', 'itself', 'couldn', \"weren't\"}\n"
     ]
    }
   ],
   "source": [
    "# Palabras de parada\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "print(stop_words_en)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "043ea8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'habido', 'tuviera', 'estaremos', 'teníamos', 'estaréis', 'erais', 'nosotras', 'del', 'estés', 'desde', 'estuvimos', 'tengamos', 'es', 'fuesen', 'tú', 'se', 'nos', 'quien', 'tendrían', 'habían', 'él', 'habríamos', 'siente', 'mías', 'estuvieses', 'sintiendo', 'seríamos', 'fuéramos', 'tendré', 'sobre', 'tuyos', 'hubieran', 'tuviese', 'sí', 'ni', 'en', 'que', 'mi', 'le', 'otros', 'por', 'mis', 'un', 'está', 'habida', 'habríais', 'míos', 'uno', 'fueseis', 'tuviésemos', 'la', 'estad', 'estemos', 'tuya', 'los', 'tuviste', 'fue', 'habrás', 'habré', 'mí', 'os', 'tengo', 'mío', 'nuestra', 'tus', 'tenemos', 'lo', 'muchos', 'vosotros', 'estamos', 'tendremos', 'tengan', 'hubiésemos', 'fuera', 'estoy', 'estando', 'de', 'sentidas', 'estarás', 'tuviesen', 'tanto', 'hay', 'vuestra', 'mía', 'estuviese', 'han', 'estuvieron', 'estada', 'estados', 'te', 'entre', 'tenga', 'seamos', 'hubo', 'tuve', 'tenido', 'estuvieran', 'habidas', 'tenidos', 'ella', 'hubiesen', 'hubisteis', 'tuvimos', 'estás', 'suyos', 'vosotras', 'sentido', 'otra', 'esta', 'estará', 'tuvo', 'nuestras', 'habrán', 'eras', 'vuestros', 'ellas', 'tuyo', 'hayan', 'e', 'más', 'hubieseis', 'habidos', 'hayáis', 'estuvieseis', 'suyas', 'fuerais', 'algunas', 'muy', 'tendrías', 'tendrás', 'estadas', 'habrías', 'estuve', 'estarían', 'teníais', 'éramos', 'porque', 'mucho', 'estuviera', 'fuese', 'estas', 'sería', 'algo', 'nuestro', 'seré', 'tenía', 'esté', 'tenida', 'tendríamos', 'hubiese', 'hubiéramos', 'todos', 'esto', 'tendrán', 'sois', 'hube', 'tenidas', 'tendría', 'fuisteis', 'pero', 'habíais', 'eres', 'no', 'estábamos', 'estar', 'será', 'estaré', 'habiendo', 'seremos', 'fueron', 'tu', 'qué', 'somos', 'serán', 'tenías', 'suyo', 'seas', 'están', 'seáis', 'tiene', 'tuyas', 'estuviesen', 'ese', 'otras', 'estén', 'estuvieras', 'nuestros', 'ya', 'habíamos', 'eso', 'hubierais', 'estuviésemos', 'tuvieseis', 'estaban', 'estaríais', 'teniendo', 'tuvisteis', 'su', 'tuvieras', 'estabais', 'hayas', 'este', 'todo', 'habéis', 'estéis', 'suya', 'habría', 'tengáis', 'esas', 'habremos', 'tenéis', 'hubiera', 'sentid', 'seréis', 'nosotros', 'habías', 'fuimos', 'hasta', 'serías', 'unos', 'habrá', 'hubiste', 'sentida', 'estaríamos', 'las', 'también', 'cuando', 'son', 'el', 'a', 'hemos', 'algunos', 'fuésemos', 'contra', 'era', 'estuviéramos', 'haya', 'fueras', 'sea', 'tuvierais', 'sus', 'tuvieses', 'eran', 'estáis', 'cual', 'yo', 'fueran', 'hubieras', 'he', 'estarán', 'otro', 'esa', 'quienes', 'tienen', 'estuviste', 'sean', 'estaba', 'seríais', 'esos', 'les', 'antes', 'estos', 'sentidos', 'tienes', 'tened', 'tuvieron', 'tuvieran', 'serás', 'estuvisteis', 'ha', 'habréis', 'fui', 'sin', 'estaría', 'soy', 'estarías', 'o', 'estuvierais', 'me', 'poco', 'vuestro', 'hubieses', 'durante', 'fuiste', 'hubimos', 'para', 'tendríais', 'serían', 'estabas', 'había', 'tendréis', 'fueses', 'hayamos', 'al', 'hubieron', 'ellos', 'nada', 'ti', 'has', 'tengas', 'estado', 'donde', 'estuvo', 'tuviéramos', 'con', 'ante', 'una', 'tenían', 'vuestras', 'y', 'habrían', 'como', 'tendrá'}\n"
     ]
    }
   ],
   "source": [
    "# Palabras de parada\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "print(stop_words_es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ac857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'demostrate', 'remove', 'stopwords', '.']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de eliminación de palabras de parada\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"This is an example to demostrate how to remove stopwords.\"\n",
    "words = word_tokenize(text)\n",
    "f_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(f_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19514b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'to', 'demostrate', 'how', 'to', 'remove', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "# eliminamos la puntuación\n",
    "import string \n",
    "f_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(f_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbb2ae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'ejemplo', 'eliminar', 'palabras', 'parada', 'español']\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo en español \n",
    "texto = \"Hola, este es un ejemplo de como eliminar palabras de parada en español.\"\n",
    "palabras = word_tokenize(texto)\n",
    "palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stop_words_es and palabra not in string.punctuation ]\n",
    "print(palabras_filtradas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "510707a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total palabras de parada en español: 313\n",
      "Total palabras de parada en inglés: 179\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total palabras de parada en español: {len(stop_words_es)}\")\n",
    "print(f\"Total palabras de parada en inglés: {len(stop_words_en)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8dea4940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/maurodelboycespedes/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/maurodelboycespedes/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a7bf0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n",
      "read\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# Lematización \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "print(lem.lemmatize(\"reading\"))\n",
    "print(lem.lemmatize(\"reading\", pos = 'v'))\n",
    "print(lem.lemmatize(\"better\", pos = 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1564559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'be', 'play', 'happily', 'in', 'the', 'park']\n"
     ]
    }
   ],
   "source": [
    "# Lematización de un texto completo\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"My dog is playing happily in the park\"\n",
    "tokens = word_tokenize(texto)\n",
    "\n",
    "lem_tokens = [lem.lemmatize(token.lower(),\"v\") for token in tokens]\n",
    "print(lem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2300d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similitud coseno - TFDF\n",
    "\n",
    "# Cuan similares llegan a ser algunas frases\n",
    "# Se encuentra entre 1 y 0\n",
    "\n",
    "# Ejemplo de aplicación\n",
    "\n",
    "import nltk\n",
    "import numpy as np \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc0291d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maurodelboycespedes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "47e085fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista de oraciones\n",
    "oraciones = [\n",
    "    \"Quantum computing is expected to revolutionize data encryption.\",\n",
    "    \"Blockchain technology is widely used in cryptocurrencies and secure transactions.\",\n",
    "    \"Artificial intelligence is transforming various industries, including healthcare and finance.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that enables computers to learn from data.\",\n",
    "    \"Neural networks mimic the human brain to process complex patterns in data.\",\n",
    "    \"Cybersecurity is crucial to protect sensitive information from hackers and cyber threats.\",\n",
    "    \"Cloud computing allows users to store and access data remotely.\",\n",
    "    \"The Internet of Things (IoT) connects everyday devices to the internet for automation.\",\n",
    "    \"Big data analytics helps businesses make data-driven decisions.\"\n",
    "]\n",
    "\n",
    "# Preprocesamiento de oraciones (volverlas en minúscula)\n",
    "oraciones= [x.lower() for x in oraciones]\n",
    "# Parámetros de procesamiento de texto\n",
    "\n",
    "max_features = 20 # Num max de palabras clave de similitud\n",
    "min_df = 1 # Palabras que como mínimo aparezca 1 vez en el texto \n",
    "max_df = 3 # Palabras que como máximo aparezca 3 veces en el texto\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Generamos el vector \n",
    "\n",
    "vectorizer =  TfidfVectorizer(max_features=max_features,\n",
    "                              min_df=min_df,\n",
    "                              max_df =max_df,\n",
    "                              stop_words=stop_words)\n",
    "\n",
    "X = vectorizer.fit_transform(oraciones).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52bee406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frases mas similares\n",
      "1: artificial intelligence is transforming various industries, including healthcare and finance.\n",
      "2: machine learning is a subset of artificial intelligence that enables computers to learn from data.\n"
     ]
    }
   ],
   "source": [
    "# Calcular la similitud coseno de todas las frases\n",
    "matriz = cosine_similarity(X)\n",
    "\n",
    "# Evitar comparar frases consigo mismas\n",
    "np.fill_diagonal(matriz,0)\n",
    "# Encontrar los indices de mayor similitud\n",
    "\n",
    "similares = np.unravel_index(np.argmax(matriz), matriz.shape)\n",
    "\n",
    "# Mostrar las dos frases mas similares \n",
    "print(\"Frases mas similares\")\n",
    "print(f\"1: {oraciones[similares[0]]}\")\n",
    "print(f\"2: {oraciones[similares[1]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
